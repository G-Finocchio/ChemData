######################################################################
## 1. COMPARISON OF LINEAR AND CONTINUOUS BERNOULLI MODELS
######################################################################
rm(list=ls())
gc()
source("tools.r")
library(ggplot2)

#######################
## Data processing
#######################

# Read data
x <- data.matrix(read.table("Data/Xtrain.txt",header=TRUE))
x.test <- data.frame(read.table("Data/Xtest.txt",header=TRUE))
y <- as.vector(read.table("Data/YTrain.txt",header=FALSE)[,1])
y.test <- as.vector(read.table("Data/YTest.txt",header=FALSE)[,1])

# Remove linear dependent columns
ind <- qr(x)$pivot[seq_len(qr(x)$rank)]
x <- x[, ind]
x.test <- x.test[,ind]
ind1 <- which(is.na(as.vector(lm(y~x)$coef)))-1
x <- x[,-ind1]
x.test <- x.test[,-ind1] 

#################################################
## Fit continuous Bernoulli and plot histogram
#################################################

# Combine data
y.all <- c(y,y.test)
n.all <- length(y.all)
tt <- 1:n.all/n.all

# Find MLE for p
ld<-function(p)
{
  (n.all*p/(2*p-1)-sum(y.all)+n.all/log((1-p)/p))/(p*(p-1))
}
p.hat <- uniroot(ld,c(0.001,0.5))$root

# Plot the histogram

ll <- log((1-p.hat)/p.hat)*p.hat^tt*(1-p.hat)^(1-tt)/(1-2*p.hat)
df <- data.frame(y.all,tt,ll)
colnames(df) <- c("Yield","Probability","Density")

pdf("yield_hist.pdf",width=10,height=10)
ggplot(df, aes(x=Yield*100)) + 
  geom_histogram(aes(y=..density..),color="black",fill="cornflowerblue",breaks=seq(0,100,length=11))+ 
  theme(text = element_text(size = 40)) +labs(title = "Yield density") +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5)))+
  geom_line(aes(x=tt*100,y =ll/100), color = "black",size=2)+ylab("Density")+xlab("Yield")
dev.off()


########################
## Linear regression
########################

fit.lm <- lm(y~x)
pred.lm <- as.matrix(x.test)%*%fit.lm$coef[-1]+fit.lm$coef[1]
rsq.lm <- as.numeric(cor(pred.lm,y.test)^2)
rmse.lm <- sqrt(mean((pred.lm*100-y.test*100)^2))

df.lm=data.frame(y.test,pred.lm)
colnames(df.lm)=c("ObservedYield","PredictedYield")

pdf("linear_model.pdf",width=10,height=10)
ggplot(data = df.lm, aes(x = PredictedYield*100, y = ObservedYield*100)) + 
  theme(text = element_text(size = 40)) +labs(title = "Linear model") +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5)))+
  geom_point(color="cornflowerblue",size=3)+
  xlab("Predicted Yield")+ylab("Observed Yield")+
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100)+
  annotate("text",x=c(-35,-28),y=c(95,89),label=c(paste("R^2:",round(rsq.lm,4)),
                                                  paste("RMSE:",round(rmse.lm,4))), size=10, parse=TRUE)
dev.off()

#################################################
### Regression with continuous Bernoulli response
#################################################

# Set starting value
beta.start <- lm(y~x)$coef

# Prediction
beta.hat <- fisher.scoring(y,x,beta.start)$beta.hat
pred.cb <- kappa1(as.vector(as.matrix(x.test)%*%beta.hat[-1]+beta.hat[1]))
rsq.cb <- as.numeric(cor(pred.cb,y.test)^2)
rmse.cb <- sqrt(mean((pred.cb*100-y.test*100)^2))

df.cb=data.frame(y.test,pred.cb)

pdf("cb_model.pdf",width=10,height=10)
ggplot(data = df.cb, aes(x = pred.cb*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = "Generalised linear model") +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5)))+
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield")+
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100)+
  annotate("text",x=c(-35,-28),y=c(95,89),label=c(paste("R^2:",round(rsq.cb,4)),
                                                  paste("RMSE:",round(rmse.cb,4))), size=10, parse=TRUE)
dev.off()



######################################################################
## 2. COMPARISON BETWEEN DUMMY CODING AND CHEMICAL DESCRIPTORS
######################################################################
rm(list=ls())
gc()
source("tools.r")
library(ggplot2)

#######################
## Data processing
#######################

# Read data
x <- data.matrix(read.table("Data/XtrainNScaled.txt",header=TRUE))
x.test <- data.frame(read.table("Data/XtestNScaled.txt",header=TRUE))
y <- as.vector(read.table("Data/YTrain.txt",header=FALSE)[,1])
y.test <- as.vector(read.table("Data/YTest.txt",header=FALSE)[,1])

y.all <- c(y,y.test)
n.all <- length(y.all)
tt <- 1:n.all/n.all

# Add three artificial descriptors for additives
xc <- rbind(x,x.test)
set.seed(2134)
c1 <- factor(xc[,1])
c2 <- factor(xc[,3])
c3 <- factor(xc[,4])
levels(c1) <- runif(22)
levels(c2) <- rnorm(22)
levels(c3) <- runif(22)
xc <- cbind(cbind(as.numeric(c1),as.numeric(c2),as.numeric(c3)),xc)
colnames(xc)[1:3] <- c("add_new1","add_new2","add_new3")

xs <- xc[,c(4,23,50,60)]
colnames(xs) <- c("additive","aryl_halide","base","ligand")

a <- rep(NA,ncol(xs))
for (i in 1:ncol(xs)) a[i] <- length(unique(xs[,i]))
xcf <- matrix(NA,nrow(xs),sum(a))
b <- cumsum(a)
colnames(xcf) <- rep(colnames(xs),times=a)
colnum <- order(unique(xs[,1]))
for (i in 2:ncol(xs)) colnum <- c(colnum,order(unique(xs[,i])))
colnames(xcf) <- paste(colnames(xcf),colnum)

for (i in 1:nrow(xs))
{
  for (j in 1:length(a))
  {
    res <- rep(0, a[j])
    where <- match( xs[i,j], unique(xs[,j]) )
    res[ where ] <- 1 
    xcf[i,(max(b[j-1],0)+1):b[j]] <- res
  }
}

for (i in 1:length(b))
{
  ind <- match(xcf[,b[i]],1)==1
  xcf[ind,(max(b[i-1],0)+1):b[i]] <- -1
}

xcf <- xcf[,-b]

xf <- xcf[1:nrow(x),]
xf.test <- xcf[-c(1:nrow(x)),]

# Mixed terms with 2-levels combinations
xx <- rep(1,nrow(xcf))
bb <- cumsum(a-1)
for (j in 1:3) {
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxp <- xcf[,i]*xcf[,-c(1:bb[j])]
    colnames(xxp) <- paste(colnames(xcf)[i],colnames(xcf[,-c(1:bb[j])]),sep=":")
    xx <- cbind(xx,xxp)
  }
}
xx <- cbind(xcf,xx[,-1])

xx.train <- xx[1:nrow(x),]
xx.test <- xx[-c(1:nrow(x)),]

# Mixed terms with 3-levels combinations
xcf1 <- xcf
colnames(xcf1) <- c(rep("additive",21),rep("aryl_halide",14),rep("base",2),rep("ligand",3))
xx1 <- xx[,-c(1:40)]

xxx <- rep(1,nrow(xcf))
ind <- rep(TRUE,ncol(xx1))
for (j in 1:2) {
  ind <- as.logical((!grepl(colnames(xcf1)[bb[j]],colnames(xx1)))*(ind))
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxxp <- xcf[,i]*xx1[,ind]
    colnames(xxxp) <- paste(colnames(xcf)[i],colnames(xx1)[ind],sep=":")
    xxx <- cbind(xxx,xxxp)
  }
}
xxx <- cbind(xx,xxx[,-1])

xxx.train <- xxx[1:nrow(x),]
xxx.test <- xxx[-c(1:nrow(x)),]

# Mixed terms with 4-levels combinations
xxx1 <- xxx[,-c(1:515)]

xxxx <- rep(NA,nrow(xcf))
for (i in 1:21) {
  xxxxp <- xcf[,i]*xxx1[,1597:1680]
  colnames(xxxxp) <- paste(colnames(xcf)[i],colnames(xxx1)[1597:1680],sep=":")
  xxxx <- cbind(xxxx,xxxxp)
}
xxxx <- cbind(xxx,xxxx[,-1])

xxxx.train <- xxxx[1:nrow(x),]
xxxx.test <- xxxx[-c(1:nrow(x)),]

#######################
## Linear models
#######################

# Fit linear models with dummy coding and descriptors matrix
fit1=lm(y.all~as.matrix(scale(xc)))
fit2=lm(y.all~xcf)
plot(fitted(fit1),fitted(fit2),type="l")

# Compare to the model with 19 descriptors
fit3=lm(y.all~scale(as.matrix(rbind(x,x.test))))
plot(fitted(fit1),fitted(fit3),type="l")

#######################
## Random forest
#######################

library(randomForest)

set.seed(1234)
# RF with 120 correlated predictors
fit.rf120 <- randomForest(y~.,data=x)
gc()

pred.rf120 <- predict(fit.rf120,x.test)
rsq.rf120 <- cor(pred.rf120,y.test)^2
rmse.rf120 <- sqrt(mean((pred.rf120*100-y.test*100)^2))

df.rf120 <- data.frame(y.test,pred.rf120)

pdf("rf-120.pdf",width=10,height=10)
ggplot(data = df.rf120, aes(x = pred.rf120*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = paste("RF with 120 predictors",sep="")) +
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield") + 
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-37,-31),y=c(95,89),label=c(paste("R^2:",round(rsq.rf120,4)),
                                                  paste("RMSE:",round(rmse.rf120,4))), size=10, parse=TRUE)
dev.off()

#RF with 39 predictors
ind <- qr(x)$pivot[seq_len(qr(x)$rank)]
x.ind <- x[, ind]
x.ind.test <- x.test[,ind]

fit.rf39 <- randomForest(y~.,data=x.ind)
gc()

pred.rf39 <- predict(fit.rf39,x.ind.test)
rsq.rf39 <- cor(pred.rf39,y.test)^2
rmse.rf39 <- sqrt(mean((pred.rf39*100-y.test*100)^2))

df.rf39 <- data.frame(y.test,pred.rf39)

pdf("rf-39.pdf",width=10,height=10)
ggplot(data = df.rf39, aes(x = pred.rf39*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = paste("RF with 39 predictors",sep="")) +
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield") + 
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-37,-31),y=c(95,89),label=c(paste("R^2:",round(rsq.rf39,4)),
                                                  paste("RMSE:",round(rmse.rf39,4))), size=10, parse=TRUE)
dev.off()

#RF with dummy coding 
fit.rf.dummy <- randomForest(y~.,data=data.frame(xf))
gc()

pred.rf.dummy <- predict(fit.rf.dummy,data.frame(xf.test))
rsq.rf.dummy <- cor(pred.rf.dummy,y.test)^2
rmse.rf.dummy <- sqrt(mean((pred.rf.dummy*100-y.test*100)^2))

df.rf.dummy <- data.frame(y.test,pred.rf.dummy)

pdf("rf-dummy.pdf",width=10,height=10)
ggplot(data = df.rf.dummy, aes(x = pred.rf.dummy*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = paste("RF with dummy predictors",sep="")) +
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield") + 
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-37,-31),y=c(95,89),label=c(paste("R^2:",round(rsq.rf.dummy,4)),
                                                  paste("RMSE:",round(rmse.rf.dummy,4))), size=10, parse=TRUE)
dev.off()

########################################################
## PLSGLM on 2-level model and most relevant predictors
########################################################

nc <- 36
beta.hat.pls2 <- plsglm.cb.simple(xx.train, y, nc, scaling=FALSE, maxit=10, tol=0.0545)
gc()

eta.hat.pls2 <- as.vector(xx.train%*%beta.hat.pls2[-1]+beta.hat.pls2[1])
fit.pls2 <- kappa1(eta.hat.pls2)
ll.pls2 <- sum(y*eta.hat.pls2-kappa0(eta.hat.pls2))

pred.pls2 <- kappa1(as.vector(xx.test%*%beta.hat.pls2[-1]+beta.hat.pls2[1]))
rsq.pls2 <- cor(pred.pls2,y.test)^2
rmse.pls2 <- sqrt(mean((pred.pls2*100-y.test*100)^2))

df.pls2 <- data.frame(y.test,pred.pls2)

filename.pls2 <- "plsglm_model2NS.pdf"
pdf(filename.pls2,width=10,height=10)
ggplot(data = df.pls2, aes(x = pred.pls2*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = paste("IRPLS-",nc," with 2 levels",sep="")) +
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield") + 
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-33,-27,-27),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls2,4)),
                                                         paste("RMSE:",round(rmse.pls2,4)),
                                                         paste("LogL:",round(ll.pls2,2))), size=10, parse=TRUE)
dev.off()

coef.pls2 <- rbind(c("intercept",beta.hat.pls2[1]),cbind(colnames(xx),beta.hat.pls2[-1]))[order(abs(beta.hat.pls2),decreasing=TRUE),]
coef.pls2[1:nc,]

############################
## GLM on 2-level model
############################

beta.hat2 <- fisher.scoring(y,xx.train,beta.hat.pls2)$beta.hat
gc()

eta.hat2 <- as.vector(xx.train%*%beta.hat2[-1]+beta.hat2[1])
fit.hat2 <- kappa1(eta.hat2)
ll.hat2 <- sum(y*eta.hat2-kappa0(eta.hat2))

pred.hat2 <- kappa1(as.vector(xx.test%*%beta.hat2[-1]+beta.hat2[1]))
rsq.hat2 <- cor(pred.hat2,y.test)^2
rmse.hat2 <- sqrt(mean((pred.hat2*100-y.test*100)^2))

df.hat2 <- data.frame(y.test,pred.hat2)

pdf("cb_model2.pdf",width=10,height=10)
ggplot(data = df.hat2, aes(x = pred.hat2*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = "GLM with 2 levels") +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield")+ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-33,-27,-27),y=c(95,89,83),label=c(paste("R^2:",round(rsq.hat2,4)),
                                                         paste("RMSE:",round(rmse.hat2,4)),
                                                         paste("LogL:",round(ll.hat2,2))), size=10, parse=TRUE)
dev.off()

rbind(c("intercept",beta.hat2[1]),cbind(colnames(xx),beta.hat2[-1]))[order(abs(beta.hat2)),]

########################################################
## PLSGLM on 3-level model and most relevant predictors
########################################################

memory.limit(32000)

nc <- 36
beta.hat.pls3 <- plsglm.cb.simple(xxx.train, y, nc, scaling=FALSE, maxit=10, tol=0.0545)
gc()

eta.hat.pls3 <- as.vector(xxx.train%*%beta.hat.pls3[-1]+beta.hat.pls3[1])
fit.pls3 <- kappa1(eta.hat.pls3)
ll.pls3 <- sum(y*eta.hat.pls3-kappa0(eta.hat.pls3))

pred.pls3 <- kappa1(as.vector(xxx.test%*%beta.hat.pls3[-1]+beta.hat.pls3[1]))
rsq.pls3 <- cor(pred.pls3,y.test)^2
rmse.pls3 <- sqrt(mean((pred.pls3*100-y.test*100)^2))

df.pls3 <- data.frame(y.test,pred.pls3)

filename.pls3 <- "plsglm_model3NS.pdf"
pdf(filename.pls3,width=10,height=10)
ggplot(data = df.pls3, aes(x = pred.pls3*100, y = y.test*100)) + theme(text = element_text(size = 40)) +
  labs(title = paste("IRPLS-",nc," with 3 levels",sep="")) +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield") + ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-33,-27,-27),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls3,4)),
                                                         paste("RMSE:",round(rmse.pls3,4)),
                                                         paste("LogL:",round(ll.pls3,2))), size=10, parse=TRUE)
dev.off()

coef.pls3 <- rbind(c("intercept",beta.hat.pls3[1]),data.frame(colnames(xxx),beta.hat.pls3[-1]))[order(abs(beta.hat.pls3),decreasing=FALSE),]
coef.pls3[2196:2186,]

############################
## GLM on 3-level model
############################

beta.hat3 <- fisher.scoring(y,xxx.train,beta.hat.pls3)$beta.hat # has convergence problems (despite clipping!)
gc()

eta.hat3 <- as.vector(xxx.train%*%beta.hat3[-1]+beta.hat3[1])
fit.hat3 <- kappa1(eta.hat3)
ll.hat3 <- sum(y*eta.hat3-kappa0(eta.hat3))

pred.hat3 <- kappa1(as.vector(xxx.test%*%beta.hat3[-1]+beta.hat3[1]))
rsq.hat3 <- cor(pred.hat3,y.test)^2
rmse.hat3 <- sqrt(mean((pred.hat3*100-y.test*100)^2))

df.hat3 <- data.frame(y.test,pred.hat3)

pdf("cb_model3.pdf",width=10,height=10)
ggplot(data = df.hat3, aes(x = pred.hat3*100, y = y.test*100)) + theme(text = element_text(size = 40)) +labs(title = "GLM with 3 levels") +
  theme(plot.title = element_textbox(hjust = 0.5, margin = margin(t = 5, b = 5)))+
  geom_point(color="cornflowerblue",size=3)+xlab("Predicted Yield")+ylab("Observed Yield")+
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed")+xlim(-50,100)+
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-35,-28),y=c(95,89),label=c(paste("R^2:",round(rsq.hat3,4)),
                                                  paste("RMSE:",round(rmse.hat3,4))), size=10, parse=TRUE)
dev.off()

rbind(c("intercept",beta.hat3[1]),data.frame(colnames(xxx.train),beta.hat3[-1]))[order(abs(beta.hat3)),]

############################
## PLSGLM on 4-level model
############################

nc <- 24
beta.hat.pls4 <- plsglm.cb.simple(xxxx.train, y, nc, scaling=FALSE, maxit=10, tol=0.0545)
gc()

eta.hat.pls4 <- as.vector(xxxx.train%*%beta.hat.pls4[-1]+beta.hat.pls4[1])
fit.pls4 <- kappa1(eta.hat.pls4)
ll.pls4 <- sum(y*eta.hat.pls4-kappa0(eta.hat.pls4))

pred.pls4 <- kappa1(as.vector(xxxx.test%*%beta.hat.pls4[-1]+beta.hat.pls4[1]))
rsq.pls4 <- cor(pred.pls4,y.test)^2
rmse.pls4 <- sqrt(mean((pred.pls4*100-y.test*100)^2))

df.pls4 <- data.frame(y.test,pred.pls4)

filename.pls4 <- "plsglm_model4NS.pdf"
pdf(filename.pls4,width=10,height=10)
ggplot(data = df.pls4, aes(x = pred.pls4*100, y = y.test*100)) + theme(text = element_text(size = 40))+
  labs(title = paste("IRPLS-",nc," with 4 levels",sep="")) +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield")+ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-33,-27,-27),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls4,4)),
                                                         paste("RMSE:",round(rmse.pls4,4)),
                                                         paste("LogL:",round(ll.pls4,2))), size=10, parse=TRUE)
dev.off()

coef.pls4 <- rbind(c("intercept",beta.hat.pls4[1]),cbind(colnames(xxxx),beta.hat.pls4[-1]))[order(abs(beta.hat.pls4),decreasing=TRUE),]
coef.pls4[1:nc,]



######################################################################
## 3. MODEL SELECTION FOR PLSGLM ON TRAIN/TEST DATA
######################################################################
rm(list=ls())
gc()
source("tools.r")
library(ggplot2)

memory.limit(64000)

#######################
## Data processing
#######################

# Read data
x <- data.matrix(read.table("Data/XtrainNScaled.txt",header=TRUE))
x.test <- data.frame(read.table("Data/XtestNScaled.txt",header=TRUE))
y <- as.vector(read.table("Data/YTrain.txt",header=FALSE)[,1])
y.test <- as.vector(read.table("Data/YTest.txt",header=FALSE)[,1])

y.all <- c(y,y.test)
n.all <- length(y.all)
tt <- 1:n.all/n.all

# Add three artificial descriptors for additives
xc <- rbind(x,x.test)
set.seed(2134)
c1 <- factor(xc[,1])
c2 <- factor(xc[,3])
c3 <- factor(xc[,4])
levels(c1) <- runif(22)
levels(c2) <- rnorm(22)
levels(c3) <- runif(22)
xc <- cbind(cbind(as.numeric(c1),as.numeric(c2),as.numeric(c3)),xc)
colnames(xc)[1:3] <- c("add_new1","add_new2","add_new3")

xs <- xc[,c(1,23,50,60)]
colnames(xs) <- c("additive","aryl_halide","base","ligand")

a <- rep(NA,ncol(xs))
for (i in 1:ncol(xs)) a[i] <- length(unique(xs[,i]))
xcf <- matrix(NA,nrow(xs),sum(a))
b <- cumsum(a)
colnames(xcf) <- rep(colnames(xs),times=a)
colnum <- order(unique(xs[,1]))
for (i in 2:ncol(xs)) colnum <- c(colnum,order(unique(xs[,i])))
colnames(xcf) <- paste(colnames(xcf),colnum)

for (i in 1:nrow(xs))
{
  for (j in 1:length(a))
  {
    res <- rep(0, a[j])
    where <- match( xs[i,j], unique(xs[,j]) )
    res[ where ] <- 1 
    xcf[i,(max(b[j-1],0)+1):b[j]] <- res
  }
}

for (i in 1:length(b))
{
  ind <- match(xcf[,b[i]],1)==1
  xcf[ind,(max(b[i-1],0)+1):b[i]] <- -1
}

xcf <- xcf[,-b]

xf <- xcf[1:nrow(x),]
xf.test <- xcf[-c(1:nrow(x)),]

# Mixed terms with 2-levels combinations
xx <- rep(1,nrow(xcf))
bb <- cumsum(a-1)
for (j in 1:3) {
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxp <- xcf[,i]*xcf[,-c(1:bb[j])]
    colnames(xxp) <- paste(colnames(xcf)[i],colnames(xcf[,-c(1:bb[j])]),sep=":")
    xx <- cbind(xx,xxp)
  }
}
xx <- cbind(xcf,xx[,-1])

xx.train <- xx[1:nrow(x),]
xx.test <- xx[-c(1:nrow(x)),]

# Mixed terms with 3-levels combinations
xcf1 <- xcf
colnames(xcf1) <- c(rep("additive",21),rep("aryl_halide",14),rep("base",2),rep("ligand",3))
xx1 <- xx[,-c(1:40)]

xxx <- rep(1,nrow(xcf))
ind <- rep(TRUE,ncol(xx1))
for (j in 1:2) {
  ind <- as.logical((!grepl(colnames(xcf1)[bb[j]],colnames(xx1)))*(ind))
  for (i in (max(bb[j-1],0)+1):bb[j]) {
    xxxp <- xcf[,i]*xx1[,ind]
    colnames(xxxp) <- paste(colnames(xcf)[i],colnames(xx1)[ind],sep=":")
    xxx <- cbind(xxx,xxxp)
  }
}
xxx <- cbind(xx,xxx[,-1])

xxx.train <- xxx[1:nrow(x),]
xxx.test <- xxx[-c(1:nrow(x)),]

# Mixed terms with 4-levels combinations
xxx1 <- xxx[,-c(1:515)]

xxxx <- rep(NA,nrow(xcf))
for (i in 1:21) {
  xxxxp <- xcf[,i]*xxx1[,1597:1680]
  colnames(xxxxp) <- paste(colnames(xcf)[i],colnames(xxx1)[1597:1680],sep=":")
  xxxx <- cbind(xxxx,xxxxp)
}
xxxx <- cbind(xxx,xxxx[,-1])

xxxx.train <- xxxx[1:nrow(x),]
xxxx.test <- xxxx[-c(1:nrow(x)),]

#######################
## PLSGLM with CV
#######################

nc <- (1:14)*3
cor2NS <- rmse2NS <- cor3NS <- rmse3NS <- cor4NS <- rmse4NS <- ll2NS <- ll3NS <- ll4NS <- rep(NA,max(nc))

for (i in nc) {
  
  print(paste("Checking",i,"comps..."))
  
  # 2-levels
  beta.hat.pls2NS <- plsglm.cb.simple(xx.train, y, i, scaling=FALSE, maxit=10, tol=0.0545)
  gc()
  pred.cb.pls2NS <- kappa1(as.vector(as.matrix(xx.test)%*%beta.hat.pls2NS[-1]+beta.hat.pls2NS[1]))
  cor2NS[i] <- cor(pred.cb.pls2NS,y.test)
  rmse2NS[i] <- sqrt(mean((pred.cb.pls2NS*100-y.test*100)^2))
  eta.hat.pls2NS <- as.vector(xx.train%*%beta.hat.pls2NS[-1]+beta.hat.pls2NS[1])
  ll2NS[i] <- sum(y*eta.hat.pls2NS-kappa0(eta.hat.pls2NS))
  
  # 3-levels
  beta.hat.pls3NS <- plsglm.cb.simple(xxx.train, y, i, scaling=FALSE, maxit=10, tol=0.0545)
  gc()
  pred.cb.pls3NS <- kappa1(as.vector(as.matrix(xxx.test)%*%beta.hat.pls3NS[-1]+beta.hat.pls3NS[1]))
  cor3NS[i] <- cor(pred.cb.pls3NS,y.test)
  rmse3NS[i] <- sqrt(mean((pred.cb.pls3NS*100-y.test*100)^2))
  eta.hat.pls3NS <- as.vector(xxx.train%*%beta.hat.pls3NS[-1]+beta.hat.pls3NS[1])
  ll3NS[i] <- sum(y*eta.hat.pls3NS-kappa0(eta.hat.pls3NS))
  
  # 4-levels
  beta.hat.pls4NS <- plsglm.cb.simple(xxxx.train, y, i, scaling=FALSE, maxit=10, tol=0.0545)
  gc()
  pred.cb.pls4NS <- kappa1(as.vector(as.matrix(xxxx.test)%*%beta.hat.pls4NS[-1]+beta.hat.pls4NS[1]))
  cor4NS[i] <- cor(pred.cb.pls4NS,y.test)
  rmse4NS[i] <- sqrt(mean((pred.cb.pls4NS*100-y.test*100)^2))
  eta.hat.pls4NS <- as.vector(xxxx.train%*%beta.hat.pls4NS[-1]+beta.hat.pls4NS[1])
  ll4NS[i] <- sum(y*eta.hat.pls4NS-kappa0(eta.hat.pls4NS))
  
}
gc()

# Save results
write(cor2NS, file="cv_cor2NS.txt", sep="\n")
write(cor3NS, file="cv_cor3NS.txt", sep="\n")
write(cor4NS, file="cv_cor4NS.txt", sep="\n")

write(rmse2NS, file="cv_rmse2NS.txt", sep="\n")
write(rmse3NS, file="cv_rmse3NS.txt", sep="\n")
write(rmse4NS, file="cv_rmse4NS.txt", sep="\n")

write(ll2NS, file="cv_ll2NS.txt", sep="\n")
write(ll3NS, file="cv_ll3NS.txt", sep="\n")
write(ll4NS, file="cv_ll4NS.txt", sep="\n")

#Load results
cor2NS <- scan(file="cv_cor2NS.txt", sep="\n")
cor3NS <- scan(file="cv_cor3NS.txt", sep="\n")
cor4NS <- scan(file="cv_cor4NS.txt", sep="\n")

rmse2NS <- scan(file="cv_rmse2NS.txt", sep="\n")
rmse3NS <- scan(file="cv_rmse3NS.txt", sep="\n")
rmse4NS <- scan(file="cv_rmse4NS.txt", sep="\n")

ll2NS <- scan(file="cv_ll2NS.txt", sep="\n")
ll3NS <- scan(file="cv_ll3NS.txt", sep="\n")
ll4NS <- scan(file="cv_ll4NS.txt", sep="\n")

# Select minimal best models
nc2NS <- min(nc[which(round((cor2NS[nc])^2,4) >= max(round((cor2NS[nc])^2,4)))])
nc3NS <- min(nc[which(round((cor3NS[nc])^2,4) >= max(round((cor3NS[nc])^2,4)))])
nc4NS <- min(nc[which(round((cor4NS[nc])^2,4) >= max(round((cor4NS[nc])^2,4)))])

# Visual results
pdf(file=paste("cv_all", ".pdf", sep=""), height=10, width=12)

df.all.rsq <- data.frame(nc=c(nc,nc,nc), 
                         type=c(rep("2-lv",times=length(nc)),rep("3-lv",times=length(nc)),rep("4-lv",times=length(nc))),
                         rsq=c(cor2NS[nc]^2,cor3NS[nc]^2,cor4NS[nc]^2))
best.rsq <- subset(df.all.rsq, (nc==nc2NS & type=="2-lv")|(nc==nc3NS & type=="3-lv")|(nc==nc4NS & type=="4-lv"))
ggplot(data = df.all.rsq, aes(x = nc, y = rsq, col=type)) + 
  geom_line(size=2) + 
  geom_point(data=best.rsq, size=8) +
  theme(text = element_text(size = 40)) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  labs(title = bquote(R^2 ~"on test data"), col="Levels") +
  xlab("Latent dimensions") + ylab(bquote(R^2 ~"test")) +
  scale_x_continuous(breaks=nc)

df.all.rmse <- data.frame(nc=c(nc,nc,nc),
                          type=c(rep("2-lv",times=length(nc)),rep("3-lv",times=length(nc)),rep("4-lv",times=length(nc))),
                          rmse=c(rmse2NS[nc],rmse3NS[nc],rmse4NS[nc]))
best.rmse <- subset(df.all.rmse, (nc==nc2NS & type=="2-lv")|(nc==nc3NS & type=="3-lv")|(nc==nc4NS & type=="4-lv"))
ggplot(data = df.all.rmse, aes(x = nc, y = rmse, col=type)) + 
  geom_line(size=2) + 
  geom_point(data=best.rmse, size=8) +
  theme(text = element_text(size = 40)) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  labs(title = "RMSE on test data", col="Levels") +
  xlab("Latent dimensions") + ylab("RMSE test") +
  scale_x_continuous(breaks=nc)

df.all.logl <- data.frame(nc=c(nc,nc,nc),
                          type=c(rep("2-lv",times=length(nc)),rep("3-lv",times=length(nc)),rep("4-lv",times=length(nc))),
                          logl=c(ll2NS[nc],ll3NS[nc],ll4NS[nc]))
best.logl <- subset(df.all.logl, (nc==nc2NS & type=="2-lv")|(nc==nc3NS & type=="3-lv")|(nc==nc4NS & type=="4-lv"))
ggplot(data = df.all.logl, aes(x = nc, y = logl, col=type)) + 
  geom_line(size=2) + 
  geom_point(data=best.logl, size=8) +
  theme(text = element_text(size = 40)) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  labs(title = "Loglike on training data", col="Levels") +
  xlab("Latent dimensions") + ylab("LogL train") +
  scale_x_continuous(breaks=nc)

dev.off()

# Best 2-levels model NS
beta.hat.pls2NS <- plsglm.cb.simple(xx.train, y, nc2NS, scaling=FALSE, maxit=10, tol=0.0545)
coef.pls2NS <- rbind(c("intercept",beta.hat.pls2NS[1]),cbind(colnames(xx),beta.hat.pls2NS[-1]))[order(abs(beta.hat.pls2NS),decreasing=TRUE),]
write.table(coef.pls2NS, file="cv_best_plsglm_model2NS_coeffs.txt", sep=" ", row.names=FALSE, col.names=FALSE)
gc()

eta.hat.pls2NS <- as.vector(xx.train%*%beta.hat.pls2NS[-1]+beta.hat.pls2NS[1])
ll.pls2NS <- sum(y*eta.hat.pls2NS-kappa0(eta.hat.pls2NS))
pred.pls2NS <- kappa1(as.vector(xx.test%*%beta.hat.pls2NS[-1]+beta.hat.pls2NS[1]))
rsq.pls2NS <- cor(pred.pls2NS,y.test)^2
rmse.pls2NS <- sqrt(mean((pred.pls2NS*100-y.test*100)^2))

df.pls2NS <- data.frame(y.test,pred.pls2NS)

pdf("cv_best_plsglm_model2NS.pdf",width=10,height=10)
ggplot(data = df.pls2NS, aes(x = pred.pls2NS*100, y = y.test*100)) + theme(text = element_text(size = 40)) +
  labs(title = paste("IRPLS-",nc2NS," on 2-levels",sep="")) +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield") + ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-35,-29,-29),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls2NS,4)),
                                                  paste("RMSE:",round(rmse.pls2NS,4)),
                                                  paste("LogL:",round(ll.pls2NS,2))), size=10, parse=TRUE)
dev.off()

# Best 3-levels model NS
beta.hat.pls3NS <- plsglm.cb.simple(xxx.train, y, nc3NS, scaling=FALSE, maxit=10, tol=0.0545)
coef.pls3NS <- rbind(c("intercept",beta.hat.pls3NS[1]),data.frame(colnames(xxx),beta.hat.pls3NS[-1]))[order(abs(beta.hat.pls3NS),decreasing=TRUE),]
write.table(coef.pls3NS, file="cv_best_plsglm_model3NS_coeffs.txt", sep=" ", row.names=FALSE, col.names=FALSE)
gc()

eta.hat.pls3NS <- as.vector(xxx.train%*%beta.hat.pls3NS[-1]+beta.hat.pls3NS[1])
ll.pls3NS <- sum(y*eta.hat.pls3NS-kappa0(eta.hat.pls3NS))
pred.pls3NS <- kappa1(as.vector(xxx.test%*%beta.hat.pls3NS[-1]+beta.hat.pls3NS[1]))
rsq.pls3NS <- cor(pred.pls3NS,y.test)^2
rmse.pls3NS <- sqrt(mean((pred.pls3NS*100-y.test*100)^2))

df.pls3NS <- data.frame(y.test,pred.pls3NS)

pdf("cv_best_plsglm_model3NS.pdf",width=10,height=10)
ggplot(data = df.pls3NS, aes(x = pred.pls3NS*100, y = y.test*100)) + theme(text = element_text(size = 40)) +
  labs(title = paste("IRPLS-",nc3NS," on 3-levels",sep="")) +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield") + ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-35,-29,-29),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls3NS,4)),
                                                         paste("RMSE:",round(rmse.pls3NS,4)),
                                                         paste("LogL:",round(ll.pls3NS,2))), size=10, parse=TRUE)
dev.off()

# Best 4-levels model NS
beta.hat.pls4NS <- plsglm.cb.simple(xxxx.train, y, nc4NS, scaling=FALSE, maxit=10, tol=0.0545)
coef.pls4NS <- rbind(c("intercept",beta.hat.pls4NS[1]),data.frame(colnames(xxxx),beta.hat.pls4NS[-1]))[order(abs(beta.hat.pls4NS),decreasing=TRUE),]
write.table(coef.pls4NS, file="cv_best_plsglm_model4NS_coeffs.txt", sep=" ", row.names=FALSE, col.names=FALSE)
gc()

eta.hat.pls4NS <- as.vector(xxxx.train%*%beta.hat.pls4NS[-1]+beta.hat.pls4NS[1])
ll.pls4NS <- sum(y*eta.hat.pls4NS-kappa0(eta.hat.pls4NS))
pred.pls4NS <- kappa1(as.vector(xxxx.test%*%beta.hat.pls4NS[-1]+beta.hat.pls4NS[1]))
rsq.pls4NS <- cor(pred.pls4NS,y.test)^2
rmse.pls4NS <- sqrt(mean((pred.pls4NS*100-y.test*100)^2))

df.pls4NS <- data.frame(y.test,pred.pls4NS)

pdf("cv_best_plsglm_model4NS.pdf",width=10,height=10)
ggplot(data = df.pls4NS, aes(x = pred.pls4NS*100, y = y.test*100)) + theme(text = element_text(size = 40)) +
  labs(title = paste("IRPLS-",nc4NS," on 4-levels",sep="")) +
  geom_point(color="cornflowerblue",size=3) + xlab("Predicted Yield") + ylab("Observed Yield") +
  geom_abline(intercept = 0, slope = 1, size = 1.5,linetype = "dashed") + xlim(-50,100) +
  theme(plot.title = element_textbox(hjust = 0.5, margin = ggplot2::margin(t = 5, b = 5))) +
  annotate("text",x=c(-35,-29,-27),y=c(95,89,83),label=c(paste("R^2:",round(rsq.pls4NS,4)),
                                                         paste("RMSE:",round(rmse.pls4NS,4)),
                                                         paste("LogL:",round(ll.pls4NS,2))), size=10, parse=TRUE)
dev.off()




