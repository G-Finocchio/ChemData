\documentclass[12pt]{article}
\usepackage{float}
\usepackage[german,english]{babel}
\usepackage{epsf}
\usepackage{latexsym}
\usepackage{epsfig}
\usepackage {amssymb}
\usepackage {amsmath}
\usepackage{color}
\setlength{\parindent}{0 cm}
\usepackage{setspace}
\usepackage{algorithm2e}


\linespread{0.95}
\def\E{\mbox{E}}
\def\O{\mathcal{O}}
\def\o{{\scriptstyle\mathcal{O}}}
\def\R{\mathbb{R}}
\def\F{{\cal{F}}}
\def\B{{\cal{B}}}
\def\X{{\cal{X}}}
\def\P{{\cal{P}}}
\def\N{{\cal{N}}}


\setlength{\textwidth}{15cm} \setlength{\textheight}{22cm}
\topmargin-1.5cm \evensidemargin0.5cm \oddsidemargin0.5cm

%\headheight0cm \headsep0cm \topskip0cm
\parindent3ex \parskip1.5ex plus 0.5ex minus 0.3ex

\usepackage{rotating}

\usepackage[longnamesfirst]{natbib}
\bibpunct{(}{)}{;}{a}{,}{,}
\renewcommand{\cite}{\citep}
\newcommand{\biblist}{
\bibliographystyle{apalike}
\bibliography{Literature}
}

\setlength{\bibsep}{0cm}

\usepackage{epsfig}
\def\beqn{\begin{eqnarray*}}
\def\eeqn{\end{eqnarray*}}
\def\beq{\begin{eqnarray}}
\def\eeq{\end{eqnarray}}
\def\bm#1{\mbox{\boldmath{$#1$}}}

\def\sumkKn{\sum_{k=1}^{K_n}}
\def\sumin{\sum_{i=1}^n}
\def\eps{\varepsilon}
\def\t{{\rm t}}
\def\var{\mbox{var}}
\def\cov{\mbox{cov}}
\newcommand{\leftsup}[2]{{\vphantom{#2}}_{#1}{#2}}

%\def\myfootnote[#1]#2{\begingroup%
%\def\thefootnote{\fnsymbol{footnote}}\footnote[#1]{#2}\endgroup}

\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}


\makeatother
\makeatletter
\renewcommand{\@fnsymbol}[1]{\@arabic{#1}}
\makeatother
\title{Supplementary material for\\
A statistical view on ``Predicting reaction performance in C--N cross-coupling using machine learning''}

\author{Tatyana Krivobokova\footnote{Department of Statistics and Operations Research,
Oskar-Morgenstern-Platz 1, 1090 Wien}\\{Universit\"at Wien}
\and Boris Maryasin\footnote{Department of Organic Chemistry, W\"ahringer Stra{\ss}e 38, 1090 Wien}\\{Universit\"at Wien}
\and Gianluca Finocchio$^1$\\{Universit\"at Wien}
} 


\begin{document}

% Algo options
\RestyleAlgo{ruled}
\SetKwComment{Comment}{/* }{ */}
\SetKw{KwIn}{\textbf{Input:}}
\SetKw{KwOut}{\textbf{Output:}}
\SetKw{And}{\textbf{and}}

\baselineskip=25pt

\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\baselineskip=20pt
\doublespacing
\linespread{1.2}


\section{Generalised linear models with continuous Bernoulli distribution }\label{suppl:sec:glm-cb}
The density of the continuous Bernoulli distribution is given by
\beqn
f(y;p)=\begin{cases}
	\frac{\log\{(1-p)/p\}}{1-2p}p^y(1-p)^{1-y},&p\in(0,1), \;p\neq 0.5\\
	2\,p^y(1-p)^{1-y},&p=0.5,
\end{cases}
\eeqn 
for $y\in[0,1]$. Rewriting
$$
f(y;p)=\exp\left[y\log\left(\frac{p}{1-p}\right)+\log(1-p)-\log(1-2p)+\log\left\{\log\left(\frac{1-p}{p}\right)\right\}\right]$$
leads to the canonical parametrisation $f(y;\eta)=\exp\{y\eta-\kappa(\eta)\}$ with
\beqn
\eta=\log\left(\frac{p}{1-p}\right)\quad\mbox{and}\quad \kappa(\eta)=\log\{\exp(\eta)-1\}-\log(\eta).
\eeqn 
This implies for $Y\sim f(y;\eta)$
\beqn
\kappa^{'}(\eta)&=&\frac{1}{1-\exp(-\eta)}-\frac{1}{\eta}=\E(Y)\\
\kappa^{''}(\eta)&=&\frac{1}{\eta^2}-\frac{\exp(\eta)}{\{1-\exp(\eta)\}^2}=\var(Y).
\eeqn 
Let now $Y=(Y_1,\ldots,Y_n)$ be a vector independent random variables, such that $Y_i$ is distributed with the density $f(y_i;\eta_i)$, $i=1,\ldots,n$. Denote also $X\in\mathbb{R}^{n\times p}$ a matrix of covariates. To link the response vector $Y$ and covariates $X$ we employ a canonical link function, that is, $g=(\kappa^{'})^{-1}$, so that $g\{\E(Y_i)\}=g\{\kappa^{'}(\eta_i)\}=\eta=X_i\beta$, where $X_i$ denotes the $i$-th row of matrix $X$. This link function is not available analytically for the continuous Bernoulli distribution. However, when the link is canonical, one does not need to know the form of $g$ for the estimation of $\beta$. %Moreover, the estimator of $\E(Y)$ one can get via $\kappa^{'}(X\hat\beta)$ instead of $g^{-1}(X\hat\beta)$.\\
The Fisher scoring algorithm for estimation of $\beta$ is given by
$$
\hat\beta_{k+1}=\hat\beta_k+F(\hat\beta_k)^{-1}S(\hat\beta_k),\;\;k=0,1,2,\ldots
$$
for $F(\beta)=X^t\mbox{diag}\{\kappa^{''}(X_1\beta),\ldots,\kappa^{''}(X_n\beta)\}X$ and $S(\beta)=X^t\{Y-\E(Y)\}$, where $\E(Y)=\{\kappa^{'}(X_1\beta),\ldots,\kappa^{'}(X_n\beta)\}^t$. \\
Once $\beta$ is estimated, the estimator for the expectation is obtained via
$$
\widehat{\E(Y_i)}=\kappa^{'}(X_i\hat\beta)=\frac{1}{1-\exp(-X_i\hat\beta)}-\frac{1}{X_i\hat\beta}.
$$
Function $\kappa^{'}(\cdot)$ is a monotone increasing function in $\hat\beta$, so that the interpretation of the regression coefficients is similar to that in linear regression.
\section{Matrix of descriptors}
\label{subsec:covariates}
First let us recall a general ANOVA model and its representation as a regression model.
For sake of simplicity consider a two-factor ANOVA model; generalisation to four factors is straightforward, but requires more complicated notations. Assume there are two factors $A$ and $B$, where factor $A$ has $I$ levels $A_1,\ldots,A_I$ and factor $B$ has $J$ levels $B_1,\ldots,B_j$. For each combination of these levels $K$ values $y_{ijk}$ is observed. The corresponding ANOVA model is given by
$$
y_{ijk}=\mu_{ij}+\epsilon_{ijk}=\mu_0+\alpha_i+\beta_j+(\alpha\beta)_{ij}+\epsilon_{ijk},\;\;i=1,\ldots,I,\;\;j=1,\ldots,J,\;\;k=1,\ldots,K,
$$
where $\mu_0$ describes an overall mean, $\alpha_i$ is a deviation of $i$-th level of factor $A$ from $\mu_0$, $\beta_j$ is a deviation of $j$-th level of factor $B$ from $\mu_0$ and $(\alpha\beta)_{ij}$ is a deviation of an interaction term. For identifiability, it is assumed that $\sum_i\alpha_i=\sum_j\beta_j=\sum_{i}(\alpha\beta)_{ij}=\sum_{j}(\alpha\beta)_{ij}=0$. 
This model can be written as a regression model 
\beqn 
y_{ijk}&=&\mu_{ij}+\epsilon_{ijk}=\mu_0+\alpha_1 x_1^A+\ldots+\alpha_{I-1}x_{I-1}^A+\beta_1x_1^B+\ldots+\beta_{J-1}x_{J-1}^b\\
&+&(\alpha\beta)_{11}x_{1,1}^{AB}+\ldots+(\alpha\beta)_{I-1,J-1}x_{I-1,J-1}^{AB}+\epsilon_{ijk},
\eeqn
where
$$
x_l^A=\begin{cases}
	1,&\mbox{for }i=1,\ldots,I-1\\
	-1,&\mbox{for }i=I\\
	0,&\mbox{otherwise}
\end{cases}
$$
$l=1,\ldots,I-1$ and 
$$
x_m^B=\begin{cases}
	1,&\mbox{for }j=1,\ldots,J-1\\
	-1,&\mbox{for }j=J\\
	0,&\mbox{otherwise}
\end{cases}
$$
$m=1,\ldots,J-1$ and $x_{l,m}^{AB}=x_l^A\cdot x_m^B$.\\
For example, for $I=2$, $J=3$ and $K=1$ one has the data
\begin{center}
	\begin{tabular}{c|c|c|c}
		&$B_1$&$B_2$&$B_3$\\\hline
		$A_1$&$y_{11}$&$y_{12}$&$y_{13}$\\
		$A_2$&$y_{21}$&$y_{22}$&$y_{23}$.
	\end{tabular}
\end{center}
Since per factor level combination there is only one observation available, the interaction term can not be estimated reliably and for the moment is assumed to be zero. The corresponding regression model without an interaction term results in
$$
Y=\left(
\begin{matrix}
	y_{11}\\
	y_{12}\\
	y_{13}\\
	y_{21}\\
	y_{22}\\
	y_{23}
\end{matrix}
\right)=\left(
\begin{matrix}
	1&1&1&0\\
	1&1&0&1\\
	1&1&-1&-1\\
	1&-1&1&0\\
	1&-1&0&1\\
	1&-1&-1&-1
\end{matrix}
\right)\left(
\begin{matrix}
	\mu_0\\
	\alpha_1\\
	\beta_1\\
	\beta_2
\end{matrix}
\right)+\left(
\begin{matrix}
	\epsilon_{11}\\
	\epsilon_{12}\\
	\epsilon_{13}\\
	\epsilon_{21}\\
	\epsilon_{23}\\
	\epsilon_{33}
\end{matrix}
\right)=:X\mu+\epsilon
$$
From the identifiability conditions one finds $\alpha_2=-\alpha_1$ and $\beta_3=-\beta_1-\beta_2$. To build interaction terms (in case $K>1$), one just needs to multiply corresponding columns of matrix $X$.\\
Let us now show that a linear model based on chemical descriptors is equivalent to an ANOVA model. For simplicity, let us consider only two factors, extension to four factors is straightforward. Let $D$ denote the $(3960\times 74)$-dimensional matrix of descriptors (both training and test sets, $3960=22\times 15\times 3\times4$), which is partitioned into a sub-matrix $D_b$ that contains $10$ descriptors of the factor {\color{blue} base} and a sub-matrix $D_l$ that contains $64$ descriptors of factor {\color{blue} ligand}. Let also $Y$ denote a $3960$-dimensional vector of yields. Then a linear model for the yield can be written as $Y=D\beta+\epsilon=D_b\beta_b+D_l\beta_l+\epsilon$, where $\beta=(\beta_b,\beta_l)$ is an unknown vector of coefficients. Now, matrix $D_b$ contains only three distinct rows, see the first five rows of $D_b$:
\begin{verbatim}
 base1  base2  base3  base4  base5  base6  base7  base8  base9 base10
 1.397  0.046  0.570 -0.157 -0.463  1.414 -0.939 -0.812 -1.056 -0.958
-0.890 -1.247 -1.406 -1.138  1.389 -0.707 -0.446 -0.597 -0.286 -0.422
-0.507  1.201  0.836  1.296 -0.926 -0.707  1.385  1.409  1.342  1.380
-0.890 -1.247 -1.406 -1.138  1.389 -0.707 -0.446 -0.597 -0.286 -0.422
-0.890 -1.247 -1.406 -1.138  1.389 -0.707 -0.446 -0.597 -0.286 -0.422
\end{verbatim}
Hence, the row rank of $D_b$ is three and therefore its column rank equals to three as well. Similarly, the rank of $D_l$ equals to four. Therefore, one can leave only first three columns of $D_b$ and first four columns of $D_l$ in the linear model. Let us denote these reduced matrices by $\widetilde{D}_b$ and $\widetilde{D}_l$, respectively, so that the linear model becomes now $Y=\widetilde{D}_b\widetilde{\beta}_b+\widetilde{D}_l\widetilde\beta_l+\epsilon$, where $\widetilde\beta_b$ contains the first three components of $\beta_b$ and $\widetilde\beta_l$ contains first four elements of $\beta_l$. Now, let us denote by $B$ a $3\times 3$ matrix, that consists of three distinct rows of $\widetilde{D}_b$ and by $L$ a $4\times 4$ matrix that consists of four distinct rows of $\widetilde{D}_l$. Both matrices are square, of full-rank and hence invertible. Therefore, 
$$
\widetilde{D}_b\widetilde{\beta}_b+\widetilde{D}_l\widetilde\beta_l=\widetilde{D}_bB^{-1}B\widetilde{\beta}_b+\widetilde{D}_lL^{-1}L\widetilde\beta_l=:X_b(B\widetilde\beta_b)+X_l(L\widetilde\beta_l)=:X_b\mu^b+X_l\mu^l,
$$
where the $i$-th row of $X_b$ is given by 
$$
X_{b,i}=\begin{cases}
	(1,0,0),&\mbox{if }\widetilde{D}_{b,i}=\widetilde{D}_{b,1}\\
	(0,1,0),&\mbox{if }\widetilde{D}_{b,i}=\widetilde{D}_{b,2}\\
	(0,0,1),&\mbox{if }\widetilde{D}_{b,i}=\widetilde{D}_{b,3},
\end{cases}
$$
with $\widetilde{D}_{b,i}$ denoting the $i$-th row of $\widetilde{D}_b$. Matrix $X_l$ is constructed in the same way and has four columns. With this, the linear model $Y=X_b\mu^b+X_l\mu^l+\epsilon$ corresponds to the ANOVA model without an interaction term
$$
\mbox{yield}_{ijk}=\mu^b_i+\mu^l_j+\epsilon_{ijk},\;\; i=1,2,3,\;\;j=1,2,3,4,\;\;k=1,\ldots,330,
$$
which can be re-parametrised to
$$
\mbox{yield}_{ijk}=\mu_0+\alpha_i+\beta_j+\epsilon_{ijk},\;\; i=1,2,3,\;\;j=1,2,3,4,\;\;k=1,\ldots,330.
$$
with the constraints $\sum_i\alpha_i=\sum_j\beta_j=0$ for identifiability. As before, $\mu_0$ is an overall mean, $\alpha_i$ is the deviation of the $i$-th level of factor {\color{blue} base} from $\mu_0$ and $\beta_j$ is the deviation of the $j$-th level of {\color{blue} ligand} from $\mu_0$. This proves our claim. For the numerical verification see R files provided.\\From these considerations it is easy to see that in general, independent of the model, the matrix of chemical descriptors is up to a linear transformation equal to a matrix $X$, which encodes with dummy variables levels of corresponding factors. Indeed, let $C=\mbox{blockdiag}(B^{-1},L^{-1})$, which is $7\times 7$ dimensional. Then, it holds that $\widetilde{D}C=(\widetilde{D}_b,\widetilde{D}_l)C=(X_b,X_l)=X$. \\
The original matrix of chemical descriptors contains only $19$ descriptors of {\color{blue} additive}, which take $22$ distinct values. It can easily be checked that the estimators based on the matrix with $19$ columns and the estimators based on $22$ columns are very close, see the R code provided. Therefore, all the results obtained with the original covariates matrix of chemical descriptors is nearly equivalent to an ANOVA model with four factors. \\
Since in practice a dummy matrix that corresponds to the given experimental design can be built directly, there is no need to use the matrix of chemical descriptors, which lacks three columns for {\color{blue} additive} and is ill-conditioned.

\section{Generalised partial least squares}
{\color{red} The PLS algorithm proposed by~\citet{Wold66} fits a linear model between a given response vector $y\in\R^n$ and a set of latent variables obtained by projecting a given covariates matrix $X\in\R^{n\times p}$ onto a suitable latent subspace. By construction, for any integer $1\leq s\leq p$, the coefficients $\widehat\beta_s$ estimated by PLS belong to the $s$-dimensional Krylov space $\mathcal{K}_s(X^\top y;X^\top X):=span\{X^\top y,(X^\top X)X^\top y,\ldots,(X^\top X)^{s-1}X^\top y\}$. One can see that the Krylov space depends on both $X$ and $y$ and accounts for the covariance between the features and the response. Let $\widehat R_s\in\R^{p\times s}$ be the orthogonal projection onto the $s$-dimensional latent space, that is, there is a unique $\widehat\alpha_s\in\R^s$ such that $\widehat\beta_s=\widehat R_s\widehat\alpha_s$. The estimated response is 
\begin{align*}
    \widehat y_s = X\widehat\beta_s = X \widehat R_s \widehat\alpha_s = \widehat T_s \widehat\alpha_s
\end{align*}
and is the result of a linear model on the latent design $\widehat T_s := X \widehat R_s$. In this sense, PLS is a supervised dimension reduction method for linear models.

The idea behind our extension of PLS to generalized linear models is straightforward, although its technicalities are nontrivial: first, it is well-known that maximum likelihood estimation in GLMs can be achieved via iteratively-reweighted-least-squares (IRLS) by \citet{mccullagh1989glm}; second, one can think of PLS as a regularization tool for solving ill-posed least-squares problems. Therefore, an iteratively-reweighted-PLS (IRPLS) algorithm seems a natural procedure to regularize maximum likelihood estimation in ill-posed GLMs. 

A pseudo-implementation is given in Algorithm~\ref{alg:irpls} below. One inputs a pair $(X,y)$ of observed data and an integer $1\leq s\leq p$, which is a latent dimension parameter. Then, the iterations of the classical IRLS algorithm are performed, but instead of solving the required weighted-least-squares problem, we fit PLS with $s$ latent components. We run the algorithm for a maximum number $J\geq0$ of iterations or until a certain threshold $\eps\geq0$ is hit by our stopping rule (likelihood ratio), both these parameters are fed to the algorithm together with the data input. Additional hyper-parameters determine whether the data should be centered/scaled and include an intercept in the coefficients. In our empirical findings, no more than $J=20$ iterations and no less than $\eps=0.05$ are ever necessary. Even in high-dimensional settings, it is often the case that $s=30$ latent components are sufficient. 

\begin{algorithm}[H]
\caption{IRPLS}\label{alg:irpls}
\KwIn{$ X, y,s,\eps,J$}\;
\KwOut{$(\widehat\beta_{s}^{(j)})_{j=0}^J$}\;
$j \gets 0$\;
$\widehat\beta_{s}^{(j)} \gets 0_p$\;
$\widehat D^{(j)} \gets \infty$\;
\While{ $\widehat D^{(j)}>1+\eps$ \And $j<J$}{
    $\widehat\eta^{(j)} \gets X\widehat\beta_{s}^{(j)}$,
    $\widehat\mu^{(j)} \gets \kappa'(\widehat\eta^{(j)})$\;
    $\widehat W^{(j)} \gets diag(\kappa''(\widehat\eta^{(j)}))$,
    $\widehat Z^{(j)} \gets \widehat\eta^{(j)}+\widehat W^{(j),-1}( y-\widehat\mu^{(j)})$\;
    $\widehat X^{(j)} \gets \widehat W^{(j),\frac{1}{2}}  X$, $\widehat y^{(j)} \gets \widehat W^{(j),\frac{1}{2}} \widehat Z^{(j)}$\; 
    $\widehat{\mathcal{K}}_s^{(j)} \gets \mathcal{K}_s^{(j)}(\widehat X^{(j),\top}\widehat y^{(j)};\widehat X^{(j),\top}\widehat X^{(j)})$;
    \begin{align}\label{eq:irpls}
        \widehat\beta_{s}^{(j+1)} \gets \arg\min_{\beta\in\widehat{\mathcal{K}}_s^{(j)}} \big\| \widehat y^{(j)} - \widehat X^{(j)}\beta \big\|_2^2;
    \end{align}\
    $\widehat D^{(j)} \gets L(\widehat\beta_{s}^{(j+1)}| X, y)/L(\widehat\beta_{s}^{(j)}| X, y)$\;
    $j \gets j+1$\;
}
\end{algorithm}

We now inspect the IRPLS algorithm in more detail. We assume we can compute or approximate with arbitrary precision the likelihood $L(\cdot|X,y)$ given the data. In particular, we have access to the auxiliary functions $\kappa'$ and $\kappa''$ which characterize the GLM setting. For continuous Bernoulli variables, these functions are given explicitly in Section~\ref{suppl:sec:glm-cb}. The algorithm is initialized at zero. Given any iteration $j\geq0$, some auxiliary quantities are defined: linear predictor $\widehat\eta^{(j)}$, predicted mean $\widehat\mu^{(j)}$, weight matrix $\widehat W^{(j)}$, pseudo-response $\widehat Z^{(j)}$, weighted design and pseudo-response $\widehat X^{(j)}$, $\widehat y^{(j)}$. Differently from the classical IRLS algorithm by~\cite{mccullagh1989glm}, we solve the weighted-least-squares problem in Equation~\eqref{eq:irpls} via PLS, which means that the coefficients $\widehat\beta_{s}^{(j+1)}$ belong to the $s$-dimensional Krylov space $\widehat{\mathcal{K}}_s^{(j)}$. Let $\widehat R_s^{(j)}\in\R^{p\times s}$ be the orthogonal projection onto the $s$-dimensional latent space, so there is a unique $\widehat\alpha_{s}^{(j+1)}\in\R^s$ such that $\widehat\beta_{s}^{(j+1)}=\widehat R_s^{(j)}\widehat\alpha_{s}^{(j+1)}$. With latent design $\widehat T_s^{(j)} := \widehat X^{(j)} \widehat R_s^{(j)}$, the estimated response is $\widehat y_s^{(j)} = \widehat X^{(j)} \widehat\beta_{s}^{(j+1)} = \widehat T_s^{(j)} \widehat\alpha_{s}^{(j+1)}$. As we can see, the dimensionality reduction is performed at each iteration and the resulting coefficients are then fed to the next cycle. We run the algorithm until the likelihood ratio stops improving (up to a small threshold) or until we reach the maximum number of steps. 

We perform model selection by dividing the data into training set $(X_{train},y_{train})$ and testing set $(X_{test},y_{test})$. After choosing a maximum number of PLS components, say $m_{pls}=42$, we run different instances of IRPLS$(X_{train},y_{train},s)$ with different hyper-parameters over all $1\leq s\leq m_{pls}$. Given one particular instance of the IRPLS algorithm, let $\widehat\beta_{s}^{(J)}$ and $\widehat y_{s}^{(J)}$ be the output coefficients and estimated response, respectively, for any given $1\leq s\leq m_{pls}$. The performance of these coefficients is measured in terms of: correlation between responses $Cor(\widehat y_{s}^{(J)},y_{test})$, root-mean-squared-error between responses $RMSE(\widehat y_{s}^{(J)},y_{test})$. Among all coefficients $\widehat\beta_1^{(J)},\ldots,\widehat\beta_{m_{pls}}^{(J)}$, those yielding the best prediction on the testing set are kept.

All computations in Algorithm~\ref{alg:irpls} are explicit, with the exception of Equation~\eqref{eq:irpls}. Our implementation in \texttt{R} is an adaptation of the \texttt{kernelpls.fit} function of the \texttt{pls} package, see~\cite{pls}. This is a numerically efficient procedure since the Krylov spaces are never computed explicitly, but only as the result of successive matrix-vector multiplications taking at most $O(np)$ operations each. That is, in order to compute the $s$-dimensional PLS solution, at most $O(snp)$ operations are required. At convergence, after $J$ maximum steps, our IRPLS algorithm has performed at most $O(Jsnp)$ operations. The above numerical properties are inherited from PLS being closely related to conjugate-gradient methods discussed in~\cite{nemirovskii1986reg,hanke1995conj}, we refer to~\cite{singer2016partial} for more details. 



}

\biblist

\end{document}
